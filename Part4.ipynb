{
  "cells": [
    {
      "metadata": {
        "_uuid": "7473ac5b0dbd647d14281deb8a34486bf444bce3"
      },
      "cell_type": "markdown",
      "source": "## **Αναγνώριση Προτύπων - 3η Εργαστηριακή Άσκηση** ##\n\n## Αναγνώριση Είδους και Εξαγωγή Συναισθήματος από Μουσική ##"
    },
    {
      "metadata": {
        "_uuid": "7ab4d25d0e02a911eb593308cce272fbd9f501b0"
      },
      "cell_type": "markdown",
      "source": "* Χρυσούλα Κοσμά - 03114025\n* Λεωνίδας Αβδελάς - 03113182\n\n9ο Εξάμηνο ΣΗΜΜΥ ΕΜΠ"
    },
    {
      "metadata": {
        "_uuid": "766fa184778e2437d7bf23d0ff0e738159123c00"
      },
      "cell_type": "markdown",
      "source": "# Βήμα 11#"
    },
    {
      "metadata": {
        "_uuid": "fc5606ff487525d427c25f7c90a5b53c49f6c75c"
      },
      "cell_type": "markdown",
      "source": "Τα συμπεράσματα του [5] είναι ότι η μεταφορά βαρών ενός νευρωνικού δικτύου (Α) σε ένα άλλο (Β) επιφέρει καλύτερα αποτελέσματα από ότι αν χρησιμοποιούσαμε τυχαία αρχικοποιημένα βάρη. Ακόμα το σημείο που θα κόψουμε το νευρωνικό Α δεν επηρεάζει κατα πολύ την βελτίωση της απόδοσης, αν αυτά τα βάρη τα προσαρμόσουμε μετά στο νέο dataset (fine-tuning)."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gzip\nimport matplotlib.pyplot as plt\nimport librosa\n\nfrom librosa import display\nfrom librosa import beat",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import copy\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import SubsetRandomSampler, DataLoader",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5972d84a31f2979fee1936ced5fd43eb6274c896"
      },
      "cell_type": "code",
      "source": "def torch_train_val_split(dataset, batch_train, batch_eval,val_size=.2, shuffle=True, seed=42):\n    # Creating data indices for training and validation splits:\n    dataset_size = len(dataset)\n    indices = list(range(dataset_size))\n    val_split = int(np.floor(val_size * dataset_size))\n    if shuffle:\n        np.random.seed(seed)\n        np.random.shuffle(indices)\n    train_indices = indices[val_split:]\n    val_indices = indices[:val_split]\n\n    # Creating PT data samplers and loaders:\n    train_sampler = SubsetRandomSampler(train_indices)\n    val_sampler = SubsetRandomSampler(val_indices)\n\n    train_loader = DataLoader(dataset,\n                              batch_size=batch_train,\n                              sampler=train_sampler)\n    val_loader = DataLoader(dataset,\n                            batch_size=batch_eval,\n                            sampler=val_sampler)\n    return train_loader, val_loader",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5f51213de58b1e3e4ca569c72c669d8bbc95aa2d"
      },
      "cell_type": "code",
      "source": "def read_spectrogram(spectrogram_file, chroma=True):\n    with gzip.GzipFile(spectrogram_file, 'r') as f:\n        spectrograms = np.load(f)\n    # spectrograms contains a fused mel spectrogram and chromagram\n    # Decompose as follows\n    return spectrograms.T",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f6b035f66c9b3db8f5407f39ab5ee265bc87f22c"
      },
      "cell_type": "code",
      "source": "class LabelTransformer(LabelEncoder):\n    def inverse(self, y):\n        try:\n            return super(LabelTransformer, self).inverse_transform(y)\n        except:\n            return super(LabelTransformer, self).inverse_transform([y])\n\n    def transform(self, y):\n        try:\n            return super(LabelTransformer, self).transform(y)\n        except:\n            return super(LabelTransformer, self).transform([y])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f0e6b24adaa15bc60f11ca31df33a87d6fd8238f"
      },
      "cell_type": "code",
      "source": "class PaddingTransform(object):\n    def __init__(self, max_length, padding_value=0):\n        self.max_length = max_length\n        self.padding_value = padding_value\n\n    def __call__(self, s):\n        if len(s) == self.max_length:\n            return s\n\n        if len(s) > self.max_length:\n            return s[:self.max_length]\n\n        if len(s) < self.max_length:\n            s1 = copy.deepcopy(s)\n            pad = np.zeros((self.max_length - s.shape[0], s.shape[1]), dtype=np.float32)\n            s1 = np.vstack((s1, pad))\n            return s1",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "288c194dda2d9e6853e5af3c423eea3dd103468d"
      },
      "cell_type": "code",
      "source": "class SpectrogramDataset(Dataset):\n    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n        self.files, labels = self.get_files_labels(self.index, class_mapping)\n        self.feats = [read_spectrogram(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        self.label_transformer = LabelTransformer()\n        if isinstance(labels, (list, tuple)):\n            self.labels = np.array(self.label_transformer.fit_transform(labels)).astype('int64')\n\n    def get_files_labels(self, txt, class_mapping):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n        files, labels = [], []\n        for l in lines:\n            label = l[1]\n            if class_mapping:\n                label = class_mapping[l[1]]\n            if not label:\n                continue\n            files.append(l[0])\n            labels.append(label)\n        return files, labels\n\n    def __getitem__(self, item):\n        l = min(self.lengths[item], self.max_length)\n        return self.zero_pad_and_stack(self.feats[item]), self.labels[item], l\n\n    def __len__(self):\n        return len(self.labels)",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a06928402ed8212286a25d501eb29ab9671de250"
      },
      "cell_type": "code",
      "source": "class MultitaskDataset(Dataset):\n    def __init__(self, path, class_mapping=None, train=True, max_length=-1):\n        t = 'train' if train else 'test'\n        p = os.path.join(path, t)\n        self.train = False\n        if train:\n            self.train = True\n            self.index = os.path.join(path, \"{}_labels.txt\".format(t))\n            self.files, labels1, labels2, labels3 = self.get_files_labels(self.index, class_mapping)\n        else:\n            self.files = os.listdir(p)\n\n        self.feats = [read_spectrogram(os.path.join(p, f)) for f in self.files]\n        self.feat_dim = self.feats[0].shape[1]\n        self.lengths = [len(i) for i in self.feats]\n        self.max_length = max(self.lengths) if max_length <= 0 else max_length\n        self.zero_pad_and_stack = PaddingTransform(self.max_length)\n        if self.train:\n            self.label_transformer = LabelTransformer()\n        \n            if isinstance(labels1, (list, tuple)):\n                self.labels1 = labels1\n            if isinstance(labels2, (list, tuple)):\n                self.labels2 = labels2\n            if isinstance(labels3, (list, tuple)):\n                self.labels3 = labels3\n\n    def get_files_labels(self, txt, class_mapping):\n        with open(txt, 'r') as fd:\n            lines = [l.rstrip().split('\\t') for l in fd.readlines()[1:]]\n            #print(lines)\n        files, labels1, labels2, labels3 = [], [], [], []\n        for l in lines:\n            chars = l[0].split(\",\")\n            #print(chars[1])\n            #print(chars[0]+'.fused.full.npy.gz')\n            files.append(chars[0]+'.fused.full.npy.gz')\n            labels1.append(float(chars[1]))\n            labels2.append(float(chars[2]))\n            labels3.append(float(chars[3]))\n        return files, labels1, labels2, labels3\n\n    def __getitem__(self, item):\n        l = min(self.lengths[item], self.max_length)\n        if self.train:\n            return self.zero_pad_and_stack(self.feats[item]), self.labels1[item], self.labels2[item], self.labels3[item], l\n        else:\n            return self.zero_pad_and_stack(self.feats[item]), l\n    def __len__(self):\n        return len(self.files)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "17f444a185dc4e266c569d928e805527cbecbd51"
      },
      "cell_type": "code",
      "source": "specs = SpectrogramDataset('../input/data/data/fma_genre_spectrograms/', train=True, class_mapping=None, max_length=-1)\ntrain_loader, val_loader = torch_train_val_split(specs, 45, 45, val_size=.33)\n",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e85ad668b764e508c51f15c60a427375402f8342"
      },
      "cell_type": "code",
      "source": "import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport math \n\nclass CNN_d2(nn.Module):\n    def __init__(self, num_classes,timesteps,num_features):\n        super(CNN_d2, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 2, kernel_size=11, stride=1, padding=5),\n            nn.BatchNorm2d(2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2)\n        )\n            \n        self.layer2 = nn.Sequential(\n            nn.Conv2d(2, 4, kernel_size=11, stride=1, padding=5),\n            nn.BatchNorm2d(4),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2))\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(4, 8, kernel_size=11, stride=1, padding=5),\n            nn.BatchNorm2d(8),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2))\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(8, 16, kernel_size=11, stride=1, padding=5),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2))\n        self.dropout = nn.Dropout(0.0001)\n        cnn_out_dim = int(math.floor(timesteps/16)*math.floor(num_features/16)*16)\n        self.fc1 = nn.Linear(cnn_out_dim, math.floor(cnn_out_dim/100))\n        self.fc2 = nn.Linear(math.floor(cnn_out_dim/100), num_classes)\n        \n    def forward(self, x):\n        #print(x.shape)\n        out = self.layer1(x)\n        #print(out.shape)\n        out = self.layer2(out)\n        #print(out.shape)\n        out = self.layer3(out)\n        #print(out.shape)\n        out = self.layer4(out)\n        #print(out.shape)\n        out = out.reshape(out.size(0), -1)\n        out = self.dropout(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f48abd73b377631ae63617541be8677e1303ba52"
      },
      "cell_type": "code",
      "source": "def eval_pred(features):\n    output_tensor = model(features.unsqueeze_(1))\n    batch_pred = torch.argmax(output_tensor.data, dim=1)\n    return batch_pred, output_tensor",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "98482647f1446d64b634f8fc7c42984c65f9de80"
      },
      "cell_type": "code",
      "source": "# Early stopping\n\nclass EarlyStopping(object):\n    \n    \"\"\"\n    EarlyStopping can be used to stop te training if no improvement after a given number of events\n    \n    Args: \n        patience(int):\n            Number of events to wait if no improvement and then stop the training\n        \n        mode(string):\n            There are two modes:\n                min, for looking for minumums\n                max, for looking for maximums\n                \n        min_delta(float):\n            The threshold of improvement\n            \n        percentage(boolean):\n            Defines whether min_delta is a percentage or an absolute number\n    \"\"\"\n    \n    def __init__(self, mode='min', min_delta=0, patience=10, percentage=False):\n        self.mode = mode\n        self.min_delta = min_delta\n        self.patience = patience\n        self.best = None\n        self.num_bad_epochs = 0 # counter of no events\n        self.is_better = None\n        self._init_is_better(mode, min_delta, percentage)\n\n        if patience == 0:\n            self.is_better = lambda a, b: True\n            self.step = lambda a: False\n\n    \"\"\"\n    Returns True if the Early Stopping has to be enforced, otherwise returns False.\n    \"\"\"\n    \n    def step(self, metrics):\n        if self.best is None:\n            self.best = metrics\n            return False\n\n        if np.isnan(metrics):\n            return True\n\n        if self.is_better(metrics, self.best):\n            self.num_bad_epochs = 0\n            self.best = metrics\n        else:\n            self.num_bad_epochs += 1\n\n        if self.num_bad_epochs >= self.patience:\n            return True\n\n        return False\n\n    def _init_is_better(self, mode, min_delta, percentage):\n        if mode not in {'min', 'max'}:\n            raise ValueError('mode ' + mode + ' is unknown!')\n        if not percentage:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - min_delta\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + min_delta\n        else:\n            if mode == 'min':\n                self.is_better = lambda a, best: a < best - (\n                            best * min_delta / 100)\n            if mode == 'max':\n                self.is_better = lambda a, best: a > best + (\n                            best * min_delta / 100)",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "798352e29305d4fc351c6f6788c798097726521e"
      },
      "cell_type": "code",
      "source": "def train_val_loop(epochs,model,criterion,optimizer,earlystopping):\n    \n    for epoch in range(epochs):\n        #train loop\n        train_loss = 0.0\n\n        for i, data in enumerate(train_loader):\n            features = torch.tensor(data[0]).float().cuda()\n            labels = torch.tensor(data[1]).long().cuda()\n\n            optimizer.zero_grad()           \n            output = model(features.unsqueeze_(1))\n\n            loss = criterion(output,labels)\n            loss.backward()\n            optimizer.step()\n            train_loss = train_loss + loss\n\n        num_batch_train = i+1\n\n        val_loss = 0.0\n        f1_val = 0.0\n\n        #validation loop\n        for j, data_val in enumerate(val_loader):\n            features_val = torch.tensor(data_val[0]).float().cuda()\n            labels_val = torch.tensor(data_val[1]).long().cuda()\n\n            batch_pred, output_tensor = eval_pred(features_val)\n\n            loss_val = criterion(output_tensor,labels_val)\n            val_loss = val_loss + loss_val\n\n            f1_val = f1_val + accuracy_score(labels_val.cpu(), batch_pred.cpu())\n\n        num_batch_val = j+1    \n        f1_val = f1_val/num_batch_val\n\n        print ('Epoch %d from %d, Train loss: %.2f' %(epoch + 1, epochs, train_loss/num_batch_train))\n        print ('Epoch %d from %d, Validation loss: %.2f' %(epoch + 1, epochs, val_loss/num_batch_val))\n        print('Score in validation set is: %d %%' % (100 * f1_val))\n        print('--------------------------------')\n        \n        if(earlystopping.step(f1_val) is True):\n            print('Early stopping the training cycle on epoch %d .' %(epoch+1))\n            break\n    return",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "6fb7c44e5b0d57fe1e320fc6bc486d47fb463c52"
      },
      "cell_type": "markdown",
      "source": "Εκπαιδεύουμε το 2D CNN του βήματος 9 για 100 εποχές με Early Stopping."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0dced10cd540b0f10898aafad0f53aad3dee80f7"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnum_classes = 20\ntimesteps = 1293\nnum_features = 140\n\nmodel = CNN_d2(num_classes,timesteps,num_features)\nmodel.cuda()\n\nprint('Training Loop for 2D CNN')\n\nepochs = 100\nLR = 0.008\nweight_decay=0.05\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), weight_decay=weight_decay, lr=LR)\nearlystopping = EarlyStopping(mode='max', min_delta=0.01, patience=8)\ntrain_val_loop(epochs,model,criterion,optimizer,earlystopping)",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training Loop for 2D CNN\nEpoch 1 from 100, Train loss: 2.90\nEpoch 1 from 100, Validation loss: 2.76\nScore in validation set is: 13 %\n--------------------------------\nEpoch 2 from 100, Train loss: 2.70\nEpoch 2 from 100, Validation loss: 2.68\nScore in validation set is: 17 %\n--------------------------------\nEpoch 3 from 100, Train loss: 2.58\nEpoch 3 from 100, Validation loss: 2.60\nScore in validation set is: 20 %\n--------------------------------\nEpoch 4 from 100, Train loss: 2.45\nEpoch 4 from 100, Validation loss: 2.55\nScore in validation set is: 20 %\n--------------------------------\nEpoch 5 from 100, Train loss: 2.39\nEpoch 5 from 100, Validation loss: 2.55\nScore in validation set is: 19 %\n--------------------------------\nEpoch 6 from 100, Train loss: 2.38\nEpoch 6 from 100, Validation loss: 2.53\nScore in validation set is: 22 %\n--------------------------------\nEpoch 7 from 100, Train loss: 2.34\nEpoch 7 from 100, Validation loss: 2.45\nScore in validation set is: 24 %\n--------------------------------\nEpoch 8 from 100, Train loss: 2.28\nEpoch 8 from 100, Validation loss: 2.47\nScore in validation set is: 24 %\n--------------------------------\nEpoch 9 from 100, Train loss: 2.25\nEpoch 9 from 100, Validation loss: 2.44\nScore in validation set is: 24 %\n--------------------------------\nEpoch 10 from 100, Train loss: 2.21\nEpoch 10 from 100, Validation loss: 2.48\nScore in validation set is: 22 %\n--------------------------------\nEpoch 11 from 100, Train loss: 2.17\nEpoch 11 from 100, Validation loss: 2.42\nScore in validation set is: 25 %\n--------------------------------\nEpoch 12 from 100, Train loss: 2.14\nEpoch 12 from 100, Validation loss: 2.38\nScore in validation set is: 27 %\n--------------------------------\nEpoch 13 from 100, Train loss: 2.11\nEpoch 13 from 100, Validation loss: 2.42\nScore in validation set is: 26 %\n--------------------------------\nEpoch 14 from 100, Train loss: 2.07\nEpoch 14 from 100, Validation loss: 2.38\nScore in validation set is: 27 %\n--------------------------------\nEpoch 15 from 100, Train loss: 2.02\nEpoch 15 from 100, Validation loss: 2.40\nScore in validation set is: 26 %\n--------------------------------\nEpoch 16 from 100, Train loss: 1.99\nEpoch 16 from 100, Validation loss: 2.43\nScore in validation set is: 25 %\n--------------------------------\nEpoch 17 from 100, Train loss: 1.96\nEpoch 17 from 100, Validation loss: 2.39\nScore in validation set is: 27 %\n--------------------------------\nEpoch 18 from 100, Train loss: 1.91\nEpoch 18 from 100, Validation loss: 2.35\nScore in validation set is: 27 %\n--------------------------------\nEpoch 19 from 100, Train loss: 1.86\nEpoch 19 from 100, Validation loss: 2.41\nScore in validation set is: 26 %\n--------------------------------\nEpoch 20 from 100, Train loss: 1.83\nEpoch 20 from 100, Validation loss: 2.34\nScore in validation set is: 30 %\n--------------------------------\nEpoch 21 from 100, Train loss: 1.79\nEpoch 21 from 100, Validation loss: 2.38\nScore in validation set is: 27 %\n--------------------------------\nEpoch 22 from 100, Train loss: 1.72\nEpoch 22 from 100, Validation loss: 2.40\nScore in validation set is: 27 %\n--------------------------------\nEpoch 23 from 100, Train loss: 1.68\nEpoch 23 from 100, Validation loss: 2.40\nScore in validation set is: 27 %\n--------------------------------\nEpoch 24 from 100, Train loss: 1.62\nEpoch 24 from 100, Validation loss: 2.41\nScore in validation set is: 27 %\n--------------------------------\nEpoch 25 from 100, Train loss: 1.59\nEpoch 25 from 100, Validation loss: 2.40\nScore in validation set is: 26 %\n--------------------------------\nEpoch 26 from 100, Train loss: 1.53\nEpoch 26 from 100, Validation loss: 2.41\nScore in validation set is: 26 %\n--------------------------------\nEpoch 27 from 100, Train loss: 1.50\nEpoch 27 from 100, Validation loss: 2.41\nScore in validation set is: 27 %\n--------------------------------\nEpoch 28 from 100, Train loss: 1.44\nEpoch 28 from 100, Validation loss: 2.39\nScore in validation set is: 28 %\n--------------------------------\nEarly stopping the training cycle on epoch 28 .\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "107f92b85a023932206b7f8086c7c0448bedd9cb"
      },
      "cell_type": "code",
      "source": "torch.save(model, \"../model.pkl\")\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Model's state_dict:\nlayer1.0.weight \t torch.Size([2, 1, 11, 11])\nlayer1.0.bias \t torch.Size([2])\nlayer1.1.weight \t torch.Size([2])\nlayer1.1.bias \t torch.Size([2])\nlayer1.1.running_mean \t torch.Size([2])\nlayer1.1.running_var \t torch.Size([2])\nlayer1.1.num_batches_tracked \t torch.Size([])\nlayer2.0.weight \t torch.Size([4, 2, 11, 11])\nlayer2.0.bias \t torch.Size([4])\nlayer2.1.weight \t torch.Size([4])\nlayer2.1.bias \t torch.Size([4])\nlayer2.1.running_mean \t torch.Size([4])\nlayer2.1.running_var \t torch.Size([4])\nlayer2.1.num_batches_tracked \t torch.Size([])\nlayer3.0.weight \t torch.Size([8, 4, 11, 11])\nlayer3.0.bias \t torch.Size([8])\nlayer3.1.weight \t torch.Size([8])\nlayer3.1.bias \t torch.Size([8])\nlayer3.1.running_mean \t torch.Size([8])\nlayer3.1.running_var \t torch.Size([8])\nlayer3.1.num_batches_tracked \t torch.Size([])\nlayer4.0.weight \t torch.Size([16, 8, 11, 11])\nlayer4.0.bias \t torch.Size([16])\nlayer4.1.weight \t torch.Size([16])\nlayer4.1.bias \t torch.Size([16])\nlayer4.1.running_mean \t torch.Size([16])\nlayer4.1.running_var \t torch.Size([16])\nlayer4.1.num_batches_tracked \t torch.Size([])\nfc1.weight \t torch.Size([102, 10240])\nfc1.bias \t torch.Size([102])\nfc2.weight \t torch.Size([20, 102])\nfc2.bias \t torch.Size([20])\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "85a2ed58c137fcf100c694869715174b01b607c5"
      },
      "cell_type": "code",
      "source": "# Model class must be defined somewhere\nmodel3 = model2 = model1 = torch.load(\"../model.pkl\")\nmodel1.eval()\nmodel2.eval()\nmodel3.eval()",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "CNN_d2(\n  (layer1): Sequential(\n    (0): Conv2d(1, 2, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n    (1): BatchNorm2d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer2): Sequential(\n    (0): Conv2d(2, 4, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer3): Sequential(\n    (0): Conv2d(4, 8, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (layer4): Sequential(\n    (0): Conv2d(8, 16, kernel_size=(11, 11), stride=(1, 1), padding=(5, 5))\n    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (dropout): Dropout(p=0.0001)\n  (fc1): Linear(in_features=10240, out_features=102, bias=True)\n  (fc2): Linear(in_features=102, out_features=20, bias=True)\n)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "fc0140c367cfe9b0491262157ffc9f58422b9b0d"
      },
      "cell_type": "markdown",
      "source": "Αλλάζουμε τις παραμέτρους εισόδου και εξόδου στις Linear συναρτήσεις ώστε να αντιστοιχούν στις εξόδους του multitask dataset. "
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "898d86460bcb0d2c89368123fb2b3217618d4d72"
      },
      "cell_type": "code",
      "source": "specs = MultitaskDataset('../input/data/data/multitask_dataset/', train=True, class_mapping=None, max_length=-1)\ntrain_loader, val_loader = torch_train_val_split(specs, 45, 45, val_size=.33)\ntest_loader = DataLoader(MultitaskDataset('../input/data/data/multitask_dataset/', train=False, class_mapping=None, max_length=-1), batch_size=45)",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5821cc011ef464fb7ac6922a8fae3aea25ec21b8"
      },
      "cell_type": "code",
      "source": "def eval_pred_multi(features, model):\n    output_tensor = model(features.unsqueeze_(1))\n    batch_pred = output_tensor.view(1,-1)[0].detach()\n    return batch_pred, output_tensor",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "11a4c41d377280baaf5a396fcdd2a2d84b8bb128"
      },
      "cell_type": "code",
      "source": "from scipy.stats import spearmanr\n\ndef train_val_loop_multi(epochs,model,criterion,optimizer,task):\n    \n    for epoch in range(epochs):\n        #train loop\n        train_loss = 0.0\n\n        for i, data in enumerate(train_loader):\n            features = torch.tensor(data[0]).float().cuda()\n            labels = torch.tensor(data[task]).float().cuda()\n            optimizer.zero_grad()           \n            output = model(features.unsqueeze_(1))\n            \n            #print(\"OUT \\n\",output.permute(1,0))\n            #print(\"Lab \\n\",labels)\n            \n            loss = criterion(output.permute(1,0),labels)\n            loss.backward()\n            optimizer.step()\n            train_loss = train_loss + loss\n\n        num_batch_train = i+1\n\n        val_loss = 0.0\n        f1_val = 0.0\n        \n        #validation loop\n        for j, data_val in enumerate(val_loader):\n            features_val = torch.tensor(data_val[0]).float().cuda()\n            labels_val = torch.tensor(data_val[task]).float().cuda()\n\n            batch_pred, output_tensor = eval_pred_multi(features_val, model)\n            \n            loss_val = criterion(output_tensor.permute(1,0),labels_val)\n            val_loss = val_loss + loss_val\n            #print(\"Batch Pred:\", batch_pred)\n            #print(\"Labels Val:\", labels_val)\n            #f1_val = f1_val + accuracy_score(labels_val.cpu(), batch_pred.cpu())\n            #corr, _ = spearmanr(labels_val.cpu().squeeze().detach().numpy(), output_tensor.cpu().permute(1,0).squeeze().detach().numpy())\n            #print(corr)\n        num_batch_val = j+1    \n        #f1_val = f1_val/num_batch_val\n        \n        print ('Epoch %d from %d, Train loss: %.4f' %(epoch + 1, epochs, train_loss/num_batch_train))\n        print ('Epoch %d from %d, Validation loss: %.4f' %(epoch + 1, epochs, val_loss/num_batch_val))\n        #print('Score in validation set is: %d %%' % (100 * f1_val))\n        print('--------------------------------')\n    return",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "464f43a49e71f7f09925b8a5fe0076d6b5ae61e3"
      },
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnum_classes = 2\ntimesteps = 1293\nnum_features = 140\n\n\ncnn_out_dim = int(math.floor(timesteps/16)*math.floor(num_features/16))*16\nmodel1.fc1 = nn.Linear(cnn_out_dim, math.floor(cnn_out_dim/100))\nmodel1.fc2 = nn.Linear(math.floor(cnn_out_dim/100), 1)\nmodel1.cuda()\n\nepochs = 20\nLR = 0.0008\nweight_decay=0.0000005\nprint('Training Loop for 2D CNN - Predictions for Valence')\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model1.parameters(), weight_decay=weight_decay, lr=LR)\ntrain_val_loop_multi(epochs,model1,criterion,optimizer,1)",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training Loop for 2D CNN - Predictions for Valence\nEpoch 1 from 20, Train loss: 0.1328\nEpoch 1 from 20, Validation loss: 0.0720\n--------------------------------\nEpoch 2 from 20, Train loss: 0.0621\nEpoch 2 from 20, Validation loss: 0.0667\n--------------------------------\nEpoch 3 from 20, Train loss: 0.0572\nEpoch 3 from 20, Validation loss: 0.0743\n--------------------------------\nEpoch 4 from 20, Train loss: 0.0570\nEpoch 4 from 20, Validation loss: 0.0670\n--------------------------------\nEpoch 5 from 20, Train loss: 0.0560\nEpoch 5 from 20, Validation loss: 0.0683\n--------------------------------\nEpoch 6 from 20, Train loss: 0.0500\nEpoch 6 from 20, Validation loss: 0.0618\n--------------------------------\nEpoch 7 from 20, Train loss: 0.0496\nEpoch 7 from 20, Validation loss: 0.0596\n--------------------------------\nEpoch 8 from 20, Train loss: 0.0510\nEpoch 8 from 20, Validation loss: 0.0644\n--------------------------------\nEpoch 9 from 20, Train loss: 0.0533\nEpoch 9 from 20, Validation loss: 0.0579\n--------------------------------\nEpoch 10 from 20, Train loss: 0.0495\nEpoch 10 from 20, Validation loss: 0.0610\n--------------------------------\nEpoch 11 from 20, Train loss: 0.0463\nEpoch 11 from 20, Validation loss: 0.0587\n--------------------------------\nEpoch 12 from 20, Train loss: 0.0446\nEpoch 12 from 20, Validation loss: 0.0598\n--------------------------------\nEpoch 13 from 20, Train loss: 0.0458\nEpoch 13 from 20, Validation loss: 0.0638\n--------------------------------\nEpoch 14 from 20, Train loss: 0.0437\nEpoch 14 from 20, Validation loss: 0.0600\n--------------------------------\nEpoch 15 from 20, Train loss: 0.0440\nEpoch 15 from 20, Validation loss: 0.0572\n--------------------------------\nEpoch 16 from 20, Train loss: 0.0409\nEpoch 16 from 20, Validation loss: 0.0590\n--------------------------------\nEpoch 17 from 20, Train loss: 0.0424\nEpoch 17 from 20, Validation loss: 0.0616\n--------------------------------\nEpoch 18 from 20, Train loss: 0.0407\nEpoch 18 from 20, Validation loss: 0.0611\n--------------------------------\nEpoch 19 from 20, Train loss: 0.0380\nEpoch 19 from 20, Validation loss: 0.0584\n--------------------------------\nEpoch 20 from 20, Train loss: 0.0408\nEpoch 20 from 20, Validation loss: 0.0644\n--------------------------------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a5a7b7fa54117f790de4772b78ba12b782b2a751"
      },
      "cell_type": "code",
      "source": "model3.fc1 = nn.Linear(cnn_out_dim, math.floor(cnn_out_dim/100))\nmodel3.fc2 = nn.Linear(math.floor(cnn_out_dim/100), 1)\nmodel3.cuda()\n\nprint('Training Loop for 2D CNN - Predictions for Danceability')\n\nepochs = 20\nLR = 0.0008\nweight_decay=0.0000005\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model3.parameters(), weight_decay=weight_decay, lr=LR)\ntrain_val_loop_multi(epochs,model3,criterion,optimizer,3)",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training Loop for 2D CNN - Predictions for Danceability\nEpoch 1 from 20, Train loss: 0.0539\nEpoch 1 from 20, Validation loss: 0.0275\n--------------------------------\nEpoch 2 from 20, Train loss: 0.0242\nEpoch 2 from 20, Validation loss: 0.0228\n--------------------------------\nEpoch 3 from 20, Train loss: 0.0228\nEpoch 3 from 20, Validation loss: 0.0244\n--------------------------------\nEpoch 4 from 20, Train loss: 0.0203\nEpoch 4 from 20, Validation loss: 0.0234\n--------------------------------\nEpoch 5 from 20, Train loss: 0.0200\nEpoch 5 from 20, Validation loss: 0.0278\n--------------------------------\nEpoch 6 from 20, Train loss: 0.0188\nEpoch 6 from 20, Validation loss: 0.0225\n--------------------------------\nEpoch 7 from 20, Train loss: 0.0193\nEpoch 7 from 20, Validation loss: 0.0225\n--------------------------------\nEpoch 8 from 20, Train loss: 0.0182\nEpoch 8 from 20, Validation loss: 0.0218\n--------------------------------\nEpoch 9 from 20, Train loss: 0.0174\nEpoch 9 from 20, Validation loss: 0.0207\n--------------------------------\nEpoch 10 from 20, Train loss: 0.0172\nEpoch 10 from 20, Validation loss: 0.0208\n--------------------------------\nEpoch 11 from 20, Train loss: 0.0167\nEpoch 11 from 20, Validation loss: 0.0205\n--------------------------------\nEpoch 12 from 20, Train loss: 0.0169\nEpoch 12 from 20, Validation loss: 0.0211\n--------------------------------\nEpoch 13 from 20, Train loss: 0.0164\nEpoch 13 from 20, Validation loss: 0.0336\n--------------------------------\nEpoch 14 from 20, Train loss: 0.0171\nEpoch 14 from 20, Validation loss: 0.0208\n--------------------------------\nEpoch 15 from 20, Train loss: 0.0162\nEpoch 15 from 20, Validation loss: 0.0201\n--------------------------------\nEpoch 16 from 20, Train loss: 0.0156\nEpoch 16 from 20, Validation loss: 0.0194\n--------------------------------\nEpoch 17 from 20, Train loss: 0.0160\nEpoch 17 from 20, Validation loss: 0.0223\n--------------------------------\nEpoch 18 from 20, Train loss: 0.0156\nEpoch 18 from 20, Validation loss: 0.0259\n--------------------------------\nEpoch 19 from 20, Train loss: 0.0153\nEpoch 19 from 20, Validation loss: 0.0197\n--------------------------------\nEpoch 20 from 20, Train loss: 0.0148\nEpoch 20 from 20, Validation loss: 0.0187\n--------------------------------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7577094b8795971c906d3ba2e8bdcd707b45d4c1"
      },
      "cell_type": "code",
      "source": "model2.fc1 = nn.Linear(cnn_out_dim, math.floor(cnn_out_dim/100))\nmodel2.fc2 = nn.Linear(math.floor(cnn_out_dim/100), 1)\nmodel2.cuda()\n\nprint('Training Loop for 2D CNN - Predictions for Energy')\n\nepochs = 20\nLR = 0.0008\nweight_decay=0.0000005\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model2.parameters(), weight_decay=weight_decay, lr=LR)\ntrain_val_loop_multi(epochs,model2,criterion,optimizer,2)",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Training Loop for 2D CNN - Predictions for Energy\nEpoch 1 from 20, Train loss: 0.0794\nEpoch 1 from 20, Validation loss: 0.0456\n--------------------------------\nEpoch 2 from 20, Train loss: 0.0473\nEpoch 2 from 20, Validation loss: 0.0539\n--------------------------------\nEpoch 3 from 20, Train loss: 0.0496\nEpoch 3 from 20, Validation loss: 0.0480\n--------------------------------\nEpoch 4 from 20, Train loss: 0.0414\nEpoch 4 from 20, Validation loss: 0.0388\n--------------------------------\nEpoch 5 from 20, Train loss: 0.0420\nEpoch 5 from 20, Validation loss: 0.0436\n--------------------------------\nEpoch 6 from 20, Train loss: 0.0353\nEpoch 6 from 20, Validation loss: 0.0410\n--------------------------------\nEpoch 7 from 20, Train loss: 0.0429\nEpoch 7 from 20, Validation loss: 0.0560\n--------------------------------\nEpoch 8 from 20, Train loss: 0.0370\nEpoch 8 from 20, Validation loss: 0.0382\n--------------------------------\nEpoch 9 from 20, Train loss: 0.0370\nEpoch 9 from 20, Validation loss: 0.0286\n--------------------------------\nEpoch 10 from 20, Train loss: 0.0296\nEpoch 10 from 20, Validation loss: 0.0367\n--------------------------------\nEpoch 11 from 20, Train loss: 0.0451\nEpoch 11 from 20, Validation loss: 0.0292\n--------------------------------\nEpoch 12 from 20, Train loss: 0.0307\nEpoch 12 from 20, Validation loss: 0.0305\n--------------------------------\nEpoch 13 from 20, Train loss: 0.0284\nEpoch 13 from 20, Validation loss: 0.0280\n--------------------------------\nEpoch 14 from 20, Train loss: 0.0295\nEpoch 14 from 20, Validation loss: 0.0304\n--------------------------------\nEpoch 15 from 20, Train loss: 0.0408\nEpoch 15 from 20, Validation loss: 0.0341\n--------------------------------\nEpoch 16 from 20, Train loss: 0.0292\nEpoch 16 from 20, Validation loss: 0.0288\n--------------------------------\nEpoch 17 from 20, Train loss: 0.0345\nEpoch 17 from 20, Validation loss: 0.0288\n--------------------------------\nEpoch 18 from 20, Train loss: 0.0332\nEpoch 18 from 20, Validation loss: 0.0280\n--------------------------------\nEpoch 19 from 20, Train loss: 0.0332\nEpoch 19 from 20, Validation loss: 0.0312\n--------------------------------\nEpoch 20 from 20, Train loss: 0.0293\nEpoch 20 from 20, Validation loss: 0.0345\n--------------------------------\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dba323a13b68087822e7ab70016aad9be0331a39"
      },
      "cell_type": "code",
      "source": "f = open('../results_transfer.txt', 'w')\nf.write('Id.fused.full.npy.gz,valence,energy,danceability')\nfor j, data_test in enumerate(test_loader):\n    #print(j)\n    features = torch.tensor(data_test[0]).float().cuda()\n    #labels_val = torch.tensor(data_test[task]).float().cuda()\n    #print(data_test)\n    batch_pred1, _ = eval_pred_multi(features, model1)\n    features = torch.tensor(data_test[0]).float().cuda()\n    batch_pred2, _ = eval_pred_multi(features, model2)\n    features = torch.tensor(data_test[0]).float().cuda()\n    batch_pred3, _ = eval_pred_multi(features, model3)\n    for i in range(len(features)):\n        file = os.listdir('../input/data/data/multitask_dataset/test')[45*j+i]\n        f.write(file + ',' + str(batch_pred1[i].cpu().numpy()) + ',' + str(batch_pred2[i].cpu().numpy()) + ',' + str(batch_pred3[i].cpu().numpy()))\nf.close()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5fc44318633028e9bf1aca1a0e4cf223b8efffd9"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}